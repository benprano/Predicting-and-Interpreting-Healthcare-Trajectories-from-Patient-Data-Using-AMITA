{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import Dict\n",
    "import transformers\n",
    "from torch import Tensor\n",
    "from torch.nn import init, Parameter\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, average_precision_score\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e19267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 1992):\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(\"Using Seed Number {}\".format(seed))\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(\n",
    "        seed\n",
    "    )  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6324411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMITA_LSTM(torch.jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size,output_dim, batch_first=True, bidirectional=True):\n",
    "        super(AMITA_LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_dim = output_dim\n",
    "        self.initializer_range=0.02\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.c1 = torch.Tensor([1]).float()\n",
    "        self.c2 = torch.Tensor([np.e]).float()\n",
    "        self.ones = torch.ones([self.input_size,1, self.hidden_size]).float()\n",
    "        self.decay_features = torch.Tensor(torch.arange(self.input_size)).float()\n",
    "        self.register_buffer('c1_const', self.c1)\n",
    "        self.register_buffer('c2_const', self.c2)\n",
    "        self.register_buffer(\"ones_const\", self.ones)\n",
    "        self.alpha = torch.FloatTensor([0.5])\n",
    "        self.alpha_imp = torch.FloatTensor([0.5])\n",
    "        self.register_buffer(\"factor\", self.alpha)\n",
    "        self.register_buffer(\"features_decay\", self.decay_features)\n",
    "        self.register_buffer(\"factor_impu\", self.alpha_imp)\n",
    "        \n",
    "        self.U_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_time = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.Dw = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        \n",
    "        self.W_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_d = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_decomp = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        \n",
    "        self.W_cell_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.W_cell_f= nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.W_cell_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        \n",
    "        \n",
    "        self.b_decomp = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_time = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_d = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        \n",
    "        #Gate Linear Unit for last records\n",
    "        self.activation_layer = nn.ELU()\n",
    "        \n",
    "        self.F_alpha_n = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size,\n",
    "                                                                                      self.hidden_size*2, 1)))\n",
    "        self.F_alpha_n_b = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size,1)))\n",
    "        self.F_beta = nn.Linear(4*self.hidden_size, 1)\n",
    "        self.Phi = nn.Linear(4*self.hidden_size, self.output_dim)\n",
    "    @torch.jit.script_method    \n",
    "    def TLSTM_unit(self, prev_hidden_memory, cell_hidden_memory, inputs, times, last_data, freq_list):\n",
    "        h_tilda_t, c_tilda_t = prev_hidden_memory, cell_hidden_memory,\n",
    "        x = inputs\n",
    "        t = times\n",
    "        l = last_data\n",
    "        freq=freq_list\n",
    "        T = self.map_elapse_time(t)\n",
    "        \n",
    "        D_ST = torch.tanh(torch.einsum(\"bij,ijk->bik\", c_tilda_t, self.W_decomp))  # Short-term memory contribution\n",
    "        # Apply temporal decay to D-STM\n",
    "        decay_factor = torch.mul(T, self.freq_decay(freq, h_tilda_t))\n",
    "        D_ST_decayed = D_ST * decay_factor\n",
    "        LTM = c_tilda_t - D_ST + D_ST_decayed  # Long-term memory contribution\n",
    "\n",
    "        # Combine short-term and long-term memory\n",
    "        c_tilda_t = D_ST_decayed + LTM\n",
    "        \n",
    "        last_tilda_t = self.activation_layer(torch.einsum(\"bij,jik->bjk\", l.unsqueeze(1), \n",
    "                                                          self.U_last)+self.b_last)\n",
    "        # Ajust previous to incoporate the latest records for each feature\n",
    "        h_tilda_t = h_tilda_t + last_tilda_t\n",
    "    \n",
    "        # Capturing Temporal Dependencies wrt to the previous hidden state\n",
    "        j_tilda_t = torch.tanh(torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_j) +\\\n",
    "                               torch.einsum(\"bij,jik->bjk\", x.unsqueeze(1),self.U_j) + self.b_j)\n",
    "        \n",
    "        # Time Gate\n",
    "        t_gate = torch.sigmoid(torch.einsum(\"bij,jik->bjk\",x.unsqueeze(1), self.U_time) + \n",
    "                               torch.sigmoid(self.map_elapse_time(t)) + self.b_time)\n",
    "        # Input Gate\n",
    "        i= torch.sigmoid(torch.einsum(\"bij,jik->bjk\",x.unsqueeze(1), self.U_i)+\\\n",
    "                         torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_i)+\\\n",
    "                         c_tilda_t*self.W_cell_i + self.b_i*self.freq_decay(freq, h_tilda_t))\n",
    "        # Forget Gate\n",
    "        f= torch.sigmoid(torch.einsum(\"bij,jik->bjk\", x.unsqueeze(1), self.U_f)+\\\n",
    "                         torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_f)+\\\n",
    "                         c_tilda_t*self.W_cell_f + self.b_f+j_tilda_t)\n",
    "\n",
    "        f_new = f * self.map_elapse_time(t) + (1 - f) *  self.freq_decay(freq, h_tilda_t)\n",
    "        # Candidate Memory Cell\n",
    "        C =torch.tanh(torch.einsum(\"bij,jik->bjk\", x.unsqueeze(1), self.U_c)+\\\n",
    "                      torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_c) + self.b_c)\n",
    "        # Current Memory Cell\n",
    "        Ct = (f_new + t_gate) * c_tilda_t + i * j_tilda_t * t_gate * C\n",
    "        # Output Gate        \n",
    "        o = torch.sigmoid(torch.einsum(\"bij,jik->bjk\",x.unsqueeze(1), self.U_o)+\n",
    "                          torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_o)+\n",
    "                          t_gate + last_tilda_t + Ct*self.W_cell_o + self.b_o)\n",
    "        # Current Hidden State\n",
    "        h_tilda_t = o * torch.tanh(Ct+last_tilda_t)\n",
    "        \n",
    "        return h_tilda_t, Ct, self.freq_decay(freq, h_tilda_t), f_new\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def map_elapse_time(self, t):\n",
    "        T = torch.div(self.c1_const, torch.log(t + self.c2_const))\n",
    "        T = torch.einsum(\"bij,jik->bjk\", T.unsqueeze(1), self.ones_const)\n",
    "        return T\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def freq_decay(self, freq_dict: torch.Tensor, ht: torch.Tensor):\n",
    "        freq_weight = torch.exp(-self.factor_impu * freq_dict)\n",
    "        weights = torch.sigmoid(torch.einsum(\"bij,jik->bjk\",freq_weight.unsqueeze(-1),self.Dw)+\\\n",
    "                                torch.einsum(\"bij,ijk->bik\", ht, self.W_d)+ self.b_d)\n",
    "        return weights\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, inputs, times, last_values, freqs):\n",
    "        device = inputs.device\n",
    "        if self.batch_first:\n",
    "            batch_size = inputs.size()[0]\n",
    "            inputs = inputs.permute(1, 0, 2)\n",
    "            last_values = last_values.permute(1, 0, 2)\n",
    "            freqs = freqs.permute(1, 0, 2)\n",
    "            times = times.transpose(0, 1)\n",
    "        else:\n",
    "            batch_size = inputs.size()[1]\n",
    "        prev_hidden = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "        prev_cell = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "       \n",
    "        seq_len = inputs.size()[0]\n",
    "        hidden_his = torch.jit.annotate(List[Tensor], [])\n",
    "        weights_decay = torch.jit.annotate(List[Tensor], [])\n",
    "        weights_fgate = torch.jit.annotate(List[Tensor], [])\n",
    "        for i in range(seq_len):\n",
    "            prev_hidden, prev_cell,pre_we_decay, fgate_f = self.TLSTM_unit(prev_hidden,prev_cell, \n",
    "                                                                           inputs[i],times[i], \n",
    "                                                                           last_values[i],freqs[i])\n",
    "            hidden_his += [prev_hidden]\n",
    "            weights_decay += [pre_we_decay]\n",
    "            weights_fgate += [fgate_f]\n",
    "        hidden_his = torch.stack(hidden_his)\n",
    "        weights_decay = torch.stack(weights_decay)\n",
    "        weights_fgate = torch.stack(weights_fgate)\n",
    "        if self.bidirectional:\n",
    "            second_hidden = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "            second_cell = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "            second_inputs = torch.flip(inputs, [0])\n",
    "            second_times = torch.flip(times, [0])\n",
    "            second_hidden_his = torch.jit.annotate(List[Tensor], [])\n",
    "            second_weights_decay = torch.jit.annotate(List[Tensor], [])\n",
    "            second_weights_fgate = torch.jit.annotate(List[Tensor], [])\n",
    "            for i in range(seq_len):\n",
    "                if i == 0:\n",
    "                    time = times[i]\n",
    "                else:\n",
    "                    time = second_times[i-1]\n",
    "                second_hidden, second_cell,b_we_decay,fgate_b = self.TLSTM_unit(second_hidden, second_cell, \n",
    "                                                                                second_inputs[i], time,\n",
    "                                                                                last_values[i],freqs[i])\n",
    "                second_hidden_his += [second_hidden]\n",
    "                second_weights_decay += [b_we_decay]\n",
    "                second_weights_fgate += [fgate_b]\n",
    "            second_hidden_his = torch.stack(second_hidden_his)\n",
    "            second_weights_fgate = torch.stack(second_weights_fgate)\n",
    "            second_weights_decay = torch.stack(second_weights_decay)\n",
    "            weights_decay =torch.cat((weights_decay, second_weights_decay), dim=-1)\n",
    "            weights_fgate =torch.cat((weights_fgate, second_weights_fgate), dim=-1)\n",
    "            hidden_his = torch.cat((hidden_his, second_hidden_his), dim=-1)\n",
    "            prev_hidden = torch.cat((prev_hidden, second_hidden), dim=-1)\n",
    "            prev_cell = torch.cat((prev_cell, second_cell), dim=-1)\n",
    "        if self.batch_first:\n",
    "            hidden_his = hidden_his.permute(1, 0, 2, 3)\n",
    "            weights_decay = weights_decay.permute(1, 0, 2, 3)\n",
    "            weights_fgate = weights_fgate.permute(1, 0, 2, 3)\n",
    "            \n",
    "        alphas = torch.tanh(torch.einsum(\"btij,ijk->btik\", hidden_his, self.F_alpha_n) + self.F_alpha_n_b)\n",
    "        alphas = torch.exp(alphas)\n",
    "        alphas = alphas/torch.sum(alphas, dim=1, keepdim=True)\n",
    "        g_n = torch.sum(alphas*hidden_his, dim=1)\n",
    "        hg = torch.cat([g_n, prev_hidden], dim=2)\n",
    "        mu = self.Phi(hg)\n",
    "        betas = torch.tanh(self.F_beta(hg))\n",
    "        betas = torch.exp(betas)\n",
    "        betas = betas/torch.max(betas, dim=1, keepdim=True).values\n",
    "        mean = torch.sum(betas*mu, dim=1)\n",
    "        return mean, alphas, betas , weights_decay, weights_fgate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(TimeLSTM, self).__init__()\n",
    "        # hidden dimensions\n",
    "        self.input_size = input_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.output_dim= output_dim\n",
    "        # Temporal embedding AMITA LSTM\n",
    "        self.amita_lstm = AMITA_LSTM(self.input_size , self.hidden_size, self.output_dim) \n",
    "    def forward(self,historic_features,timestamp, last_features, features_freqs , is_test=False):\n",
    "        # Temporal features embedding\n",
    "        outputs, alphas, betas, decay_weights, fgate = self.amita_lstm(historic_features,timestamp, \n",
    "                                                                    last_features, features_freqs)\n",
    "        if is_test:\n",
    "            return alphas, betas, decay_weights, fgate, outputs\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, mode, path, patience=3, delta=0):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError(\"Argument mode must be one of 'min' or 'max'.\")\n",
    "        if patience <= 0:\n",
    "            raise ValueError(\"Argument patience must be a positive integer.\")\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"Argument delta must not be a negative number.\")\n",
    "\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = np.inf if mode == 'min' else -np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def _is_improvement(self, val_score):\n",
    "        \"\"\"Return True iff val_score is better than self.best_score.\"\"\"\n",
    "        if self.mode == 'max' and val_score > self.best_score + self.delta:\n",
    "            return True\n",
    "        elif self.mode == 'min' and val_score < self.best_score - self.delta:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        \"\"\"\n",
    "        Return True iff self.counter >= self.patience.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._is_improvement(val_score):\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(\"Val loss improved, Saving model's best weights.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'Early stopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                print(f'Stopped early. Best val loss: {self.best_score:.4f}')\n",
    "                return True\n",
    "\n",
    "\n",
    "class TrainerHelpers:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device, optim, loss_criterion, schedulers, num_epochs, patience_n=50, task=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        self.optim = optim\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.schedulers = schedulers\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience_n = patience_n\n",
    "        self.task = task\n",
    "\n",
    "    @staticmethod\n",
    "    def acc(predicted, label):\n",
    "        predicted = predicted.sigmoid()\n",
    "        pred = torch.round(predicted.squeeze())\n",
    "        return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "    def train_model(self, model, train_dataloader):\n",
    "        modele.train()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        for bi, inputs in enumerate(tqdm(train_dataloader, total=len(train_dataloader), leave=False)):\n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            self.optim.zero_grad()\n",
    "            _, _, _, _, outputs = model(temporal_features, timestamp,\n",
    "                                        last_data, data_freqs, is_test=True)\n",
    "            if self.task:\n",
    "                loss = self.loss_criterion(outputs.sigmoid().squeeze(-1), labels.squeeze(-1))\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += self.acc(outputs, labels)\n",
    "            else:\n",
    "                loss = self.loss_criterion(outputs.squeeze(-1), labels.squeeze(-1))\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                running_loss += loss.item()\n",
    "        if self.task:\n",
    "            epoch_loss = running_loss / len(train_dataloader)\n",
    "            epoch_acc = running_corrects / len(train_dataloader.dataset)\n",
    "            return epoch_loss, epoch_acc\n",
    "        else:\n",
    "            return running_loss / len(train_dataloader)\n",
    "\n",
    "    def valid_model(self, model, valid_dataloader):\n",
    "        model.eval()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        fin_targets, fin_outputs = [], []\n",
    "        for bi, inputs in enumerate(tqdm(valid_dataloader, total=len(valid_dataloader), leave=False)):\n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, _, _, _, outputs = model(temporal_features, timestamp,\n",
    "                                            last_data, data_freqs, is_test=True)\n",
    "            if self.task:\n",
    "                loss = self.loss_criterion(outputs.sigmoid().squeeze(-1), labels.squeeze(-1))\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += self.acc(outputs, labels)\n",
    "            else:\n",
    "                loss = self.loss_criterion(outputs.squeeze(-1), labels.squeeze(-1))\n",
    "                running_loss += loss.item()\n",
    "            fin_targets.append(labels.cpu().detach().numpy())\n",
    "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "        if self.task:\n",
    "            epoch_loss = running_loss / len(valid_dataloader)\n",
    "            epoch_acc = running_corrects / len(valid_dataloader.dataset)\n",
    "            return epoch_loss, epoch_acc, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "        else:\n",
    "            mse = mean_squared_error(np.vstack(fin_targets), np.vstack(fin_outputs))\n",
    "            mae = mean_absolute_error(np.vstack(fin_targets), np.vstack(fin_outputs))\n",
    "            return running_loss / len(valid_dataloader), mse, mae, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "\n",
    "    def eval_model(self, model_class, model_path,  test_dataloader):\n",
    "        # Initialize the model architecture\n",
    "        model = model_class(self.input_dim, self.hidden_dim, self.output_dim).to(self.device)\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        fin_targets, fin_outputs = [], []\n",
    "        all_alphas, all_betas, all_decays, fgate_weights = [], [], [], []\n",
    "        for bi, inputs in enumerate(tqdm(test_dataloader, total=len(test_dataloader), leave=False, \n",
    "                                         desc='Evaluating on test data')):\n",
    "            \n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                alphas, betas, decay_weights, fgate, outputs = model(temporal_features, timestamp,\n",
    "                                                                     last_data, data_freqs, is_test=True)\n",
    "            if self.task:\n",
    "                fin_outputs.append(outputs.sigmoid().cpu().detach().numpy())\n",
    "                \n",
    "            else:\n",
    "                fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "            fin_targets.append(labels.cpu().detach().numpy())\n",
    "            all_alphas.append(alphas.cpu().detach().numpy())\n",
    "            all_betas.append(betas.cpu().detach().numpy())\n",
    "        return all_alphas, all_betas, all_decays, fgate_weights, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "\n",
    "    def train_validate_evaluate(self,model_class,  model, model_name, train_loader, val_loader, test_loader, params, model_path):\n",
    "        best_losses, all_scores = [], []\n",
    "        es = EarlyStopping(mode='min', path=f\"{os.path.join(model_path, f'model_{model_name}.pth')}\",\n",
    "                           patience=self.patience_n)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if self.task:\n",
    "                loss, accuracy = self.train_model(model, train_loader)\n",
    "                eval_loss, eval_accuracy, __, _ = self.valid_model(model, val_loader)\n",
    "                if self.schedulers is not None:\n",
    "                    self.schedulers.step()\n",
    "                print(\n",
    "                    f\"lr: {self.optim.param_groups[0]['lr']:.7f}, epoch: {epoch + 1}/{self.num_epochs}, train loss: {loss:.8f}, accuracy: {accuracy:.8f} | valid loss: {eval_loss:.8f}, accuracy: {eval_accuracy:.4f}\")\n",
    "                if es(eval_loss, model):\n",
    "                    best_losses.append(es.best_score)\n",
    "                    print(\"best_score\", es.best_score)\n",
    "                    break\n",
    "            else:\n",
    "                loss = self.train_model(model, train_loader)\n",
    "                eval_loss, mse_loss, mae_loss, _, _ = self.valid_model(model, val_loader)\n",
    "                if self.schedulers is not None:\n",
    "                    self.schedulers.step()\n",
    "                print(\n",
    "                    f\"lr: {self.optim.param_groups[0]['lr']:.7f}, epoch: {epoch + 1}/{self.num_epochs}, train loss: {loss:.8f} | valid loss: {eval_loss:.8f} valid mse loss: {mse_loss:.8f}, valid mae loss: {mae_loss:.8f}\")\n",
    "                if es(mse_loss, model):\n",
    "                    best_losses.append(es.best_score)\n",
    "                    print(\"best_score\", es.best_score)\n",
    "                    break\n",
    "        if self.task:\n",
    "            _, _, y_true, y_pred = self.valid_model(model, val_loader)\n",
    "            pr_score = average_precision_score(y_true, y_pred)\n",
    "            print(f\"[INFO] PR-AUC ON FOLD :{model_name} -  score val data: {pr_score:.4f}\")\n",
    "        else:\n",
    "            _, _, _, y_true, y_pred = self.valid_model(modele, val_loader)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            print(\n",
    "                f\"[INFO] mse loss & mae loss on validation data Fold {model_name}: mse loss: {mse:.4f} - mae loss: {mae:.4f}\")\n",
    "        if self.task:\n",
    "            f1_scores_folds = []\n",
    "            targets, outputs = self._evaluate_model(model_class, f\"{os.path.join(model_path, f'model_{model_name}.pth')}\",\n",
    "                                                    test_loader)\n",
    "            scores = self.metrics_binary(targets, outputs)\n",
    "            delta, f1_scr = self.best_threshold(np.vstack(targets), np.vstack(outputs))\n",
    "            f1_scores_folds.append((delta, f1_scr))\n",
    "            all_scores.append((scores, f1_scores_folds))\n",
    "            np.savez(os.path.join(model_path, f\"results_data_{model_name}.npz\"), \n",
    "                         auc_pr=scores, true_labels_data=np.vstack(outputs), \n",
    "                         predicted_labels_data= np.vstack(targets), \n",
    "                         folds_f1_scores= f1_scores_folds)\n",
    "            print(f\"[INFO] Results on test Folds {all_scores}\")\n",
    "        else:\n",
    "            targets, outputs = self._evaluate_model(modele_class, f\"{os.path.join(model_path, f'model_{model_name}.pth')}\",\n",
    "                                                    test_loader)\n",
    "            scores = self.metrics_reg(targets, outputs, params)\n",
    "            all_scores.append(scores)\n",
    "            np.savez(os.path.join(model_path, f\"test_data_fold_{model_name}.npz\"), \n",
    "                      reg_scores=scores,true_labels=targets, predicted_labels= outputs)\n",
    "            print(f\"[INFO] Results on test Folds {all_scores}\")\n",
    "        return all_scores\n",
    "\n",
    "    def _evaluate_model(self, model, model_path,  test_dataloader):\n",
    "        targets, predicted = [], []\n",
    "        all_alphas, all_betas, all_decays, fgate_weights = [], [], [], []\n",
    "        alphas, betas, _, _, y_pred, y_true = self.eval_model(model, model_path, test_dataloader)\n",
    "        targets.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "        all_alphas.append(alphas)\n",
    "        all_betas.append(betas)\n",
    "        targets_all = [np.vstack(targets[i]) for i in range(len(targets))]\n",
    "        predicted_all = [np.vstack(predicted[i]) for i in range(len(predicted))]\n",
    "        return targets_all, predicted_all\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics_binary(targets, predicted):\n",
    "        scores = []\n",
    "        for y_true, y_pred, in zip(targets, predicted):\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_pred, y_true)\n",
    "            auc_score = metrics.auc(fpr, tpr)\n",
    "            pr_score = metrics.average_precision_score(y_pred, y_true)\n",
    "            scores.append([np.round(np.mean(auc_score), 4),\n",
    "                           np.round(np.mean(pr_score), 4)])\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def best_threshold(y_train,train_preds):\n",
    "        delta, tmp = 0, [0, 0, 0]  # idx, cur, max\n",
    "        for tmp[0] in tqdm(np.arange(0.1, 1.01, 0.01)):\n",
    "            tmp[1] = f1_score(train_preds, np.array(y_train) > tmp[0])\n",
    "            if tmp[1] > tmp[2]:\n",
    "                delta = tmp[0]\n",
    "                tmp[2] = tmp[1]\n",
    "        print('best threshold is {:.2f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "        return delta, tmp[2]\n",
    "\n",
    "    @staticmethod\n",
    "    def adjusted_r2(actual: np.ndarray, predicted: np.ndarray, rowcount: np.int64, featurecount: np.int64):\n",
    "        return 1 - (1 - r2_score(actual, predicted)) * (rowcount - 1) / (rowcount - featurecount)\n",
    "\n",
    "    def metrics_reg(self, targets, predicted, rescale_params):\n",
    "        scores = []\n",
    "        for y_true, y_pred, in zip(targets, predicted):\n",
    "            target_max, target_min = rescale_params['data_targets_max'], rescale_params['data_targets_min']\n",
    "            targets_y_true = y_true * (target_max - target_min) + target_min\n",
    "            targets_y_pred = y_pred * (target_max - target_min) + target_min\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            n = y_true.shape[0]\n",
    "            r2 = r2_score(targets_y_true, targets_y_pred)\n",
    "            adj_r2 = self.adjusted_r2(targets_y_true, targets_y_pred, n, self.input_dim)\n",
    "            scores.append([rmse, mae, r2, adj_r2])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544232a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn=\"/media/sangaria/8TB-FOLDERS/PAPER_REVIEWS_DATASETS_TASKS/24_HOURS_DATA/\"\n",
    "task_dataset =\"ICU_MORTALITY_ALL_FEATURES_24_HRS_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_loader = np.load(os.path.join(os.path.join(dn,task_dataset),\n",
    "                                          \"train_test_data.npz\"), \n",
    "                                          allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149faa3",
   "metadata": {},
   "source": [
    "train_val_loader = all_dataset_loader['folds_data_train_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ae263",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_loader.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a12c0",
   "metadata": {},
   "source": [
    "test_loader = all_dataset_loader['folds_data_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_settings = np.load(os.path.join(os.path.join(dn,task_dataset),\n",
    "                                        \"data_max_min.npz\"), \n",
    "                                         allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5172475",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_settings.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c37020",
   "metadata": {},
   "source": [
    "dataset_settings['data_targets_max'], dataset_settings['data_targets_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max, data_min= dataset_settings['data_max'], dataset_settings['data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##device = 'cuda:1' \n",
    "seq_length = dataset_settings['seq_length'].item()\n",
    "input_dim = dataset_settings['input_dim'].item()\n",
    "hidden_dim, output_dim  = 64, 1\n",
    "seq_length, input_dim, hidden_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ce232",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "optimizer_config={\"lr\": 1e-3, \"betas\": (0.9, 0.98), \"eps\": 4e-09, \"weight_decay\": 5e-4}\n",
    "NUM_EPOCHS =5\n",
    "NUM_FOLDS = len(train_loader)\n",
    "model_name=\"TIME_AWARE_LSTM\".lower()\n",
    "n_patience = 5\n",
    "batch_size=64\n",
    "steps_per_epoch = int(dataset_settings['shape_data'][0] / batch_size / NUM_FOLDS)\n",
    "total_steps_per_fold = int(steps_per_epoch * NUM_EPOCHS)\n",
    "num_warmup_steps = int(0.1 * total_steps_per_fold)\n",
    "num_warmup_steps,total_steps_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6a7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amita = TimeLSTM(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(amita.parameters(), **optimizer_config)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n",
    "                                                               num_warmup_steps=num_warmup_steps, \n",
    "                                                               num_training_steps=total_steps_per_fold, \n",
    "                                                               num_cycles=2)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "#criterion = nn.BCELoss().to(device)\n",
    "best_metric = 0.0\n",
    "best_model_wts = deepcopy(amita.state_dict())\n",
    "amita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_inference = TrainerHelpers(input_dim, hidden_dim, output_dim,\n",
    "                                       device, optimizer, criterion, \n",
    "                                       scheduler, NUM_EPOCHS, \n",
    "                                       patience_n=n_patience, task=False)\n",
    "train_valid_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_data_used = seq_length\n",
    "taskname=f\"ICU_LOS_HELPER_TASKS_{hours_data_used}_HOURS_DATA\"\n",
    "main_path =  \"/home/sangaria/Videos/Second journal paper/REVIEWS/AMITA_LSTM/RESULTS\"\n",
    "task_path=f\"{os.path.join(main_path, f'{taskname}')}\"\n",
    "if not os.path.exists(task_path):\n",
    "    os.makedirs(task_path)\n",
    "task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35007ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_folds= []\n",
    "for idx, (train_loader, test_data) in enumerate(zip(train_val_loader ,test_loader)):\n",
    "    print(f'[INFO]: Training on fold : {idx+1}')\n",
    "    # Reset the model weights\n",
    "    amita.load_state_dict(best_model_wts)\n",
    "    train_data, valid_data= train_loader\n",
    "    scores= train_valid_inference.train_validate_evaluate(TimeLSTM, amita, idx+1,train_data,valid_data,\n",
    "                                                          test_data, dataset_settings, task_path)\n",
    "    scores_folds.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed839940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
